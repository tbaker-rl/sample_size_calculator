{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7be174",
   "metadata": {},
   "source": [
    "# Power Analysis\n",
    "\n",
    "The power to reject the null hypothesis\n",
    "\n",
    "1. Determine the Power of the test:\n",
    "- probability of correctly rejecting the null hypothesis\n",
    "- probability of not making a type II error (1 - beta)\n",
    "- Beta: type II error\n",
    "- common to pick 80% as the power of the A/B test\n",
    "\n",
    "This means that we are fine with not detecting a true treatment effect 20% of the time when there is a treatment effect (meaning we have failed to reject the null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d491fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "13c7dc2a",
   "metadata": {},
   "source": [
    "# Significance Level\n",
    "\n",
    "2. Determine the significance level of the test\n",
    "- probability of rejecting the null while the null is true\n",
    "- detecting statistically significance while its not\n",
    "- probability of making a type 1 error (alpha)\n",
    "- common to pick 5% bas significance level of the test\n",
    "\n",
    "probability of making a false discovery - that is a false positive rate (saying there is an effect when there isnt). Using 5% means there is a 5% risk that there isnt a difference, but we say there is. This translates to a 95% confidence in any affect found (5% risk of false positive type 1 error)\n",
    "\n",
    "You can vary the significance level - make it bigger if running the AB test is related to high engineering costs, then the business might decide to choose a high alpha, so that it will be easier to detect a treatment effect. However if the implementation cost of the proposed change is high, we want to set a small alpha to be really really sure that we can reject the null and have seen a change (making it harder to reject the null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b2ae1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ac0b047",
   "metadata": {},
   "source": [
    "#  Minimum Detectable Effect\n",
    "\n",
    "3. Determin the minimum detectable effect\n",
    "\n",
    "- What is the substantive to the statistical significance for business to find this investment worth it - that this feature is worth to launch into production\n",
    "- proxy that relates to the smallest effect that would matter in practice\n",
    "- No common level here - this depends on the business ask - usually set by product / \n",
    "\n",
    "What is the % increase we want to see in the product to make this new release or new product / change worth it to the business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2450e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "208e3861",
   "metadata": {},
   "source": [
    "# Power Analysis\n",
    "\n",
    "1. Beta - Probability of type II error\n",
    "2. (1-Beta) - Power of the test\n",
    "3. Alpha - Porbability of Type 1 error, Significance level\n",
    "4. Delta - the minimum desired effect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a4285d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7350cfc",
   "metadata": {},
   "source": [
    "# Calculating the minimum sample size\n",
    "\n",
    "To make sure our results are are repeatable, robust, and can be generalised to the entire population, we need to avoid P hacking, to ensure that real statistical significance and to avoid bias results. \n",
    "\n",
    "So we want to make sure that we collect enough observations and we run the test for a minimum predetermined period of time.\n",
    "\n",
    "Therefore before running the test  we need to determine the sample size of the control and experiment groups, as well as the test duration.\n",
    "\n",
    "This is calculated using the defined:\n",
    "- power of the test, \n",
    "- the significance level \n",
    "- as well as the mimum desired effect.\n",
    "\n",
    "There are two possabilities here -\n",
    "1. The primary metric is in the form of a binary variable\n",
    "2. The primary metric is in the form of proportions or averages\n",
    "\n",
    "For example 1 would be conversion vs no conversion, click vs no click etc.\n",
    "\n",
    "For example 2 this would be mean click through rate, mean completion rate etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83615729",
   "metadata": {},
   "source": [
    "For means, this would be:\n",
    "H0 (null hypothesis): mean(con) = mean(exp)\n",
    "H1 (alternative hypothesis): mean(con) != mean(exp)\n",
    "\n",
    "This is based on the core central limit theorum and that the means of both groups follow a normal distribution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f1d7f",
   "metadata": {},
   "source": [
    "https://clincalc.com/stats/samplesize.aspx\n",
    "\n",
    "Plug in your values for the means, Alpha, Power and continuous / dichotomous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f58013",
   "metadata": {},
   "source": [
    "# A/B test Duriation\n",
    "\n",
    "This needs to be calculated before the experiment is run, otherwise you can be in danger of stopping the test when you detect statistical significance, which is known as p hacking, and is not what we want to do\n",
    "\n",
    "Duration = Minimum sample size / # visitors per day\n",
    "\n",
    "Dangers:\n",
    "- Too small a test duration -> novelty effect, users tend to react positively to new changes / new products that wear off over time\n",
    "- Test duration to large -> maturation effects, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef8a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05 #probabilty of type 1 error, the possibility of rejecting the null hypothesis when its true, we are comfortable making this mistake 5% of the time\n",
    "delta = 0.1 #Delta, minimum desired effect\n",
    "power = 0.8 # 1 - (the probabilty of type 2 error, false negative rate, number of times we end up failing to reject the null when its false)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba51318",
   "metadata": {},
   "source": [
    "# Test Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e4f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (2.3.4)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: numpy<2.6,>=1.25.2 in ./.venv/lib/python3.12/site-packages (from scipy) (2.3.4)\n",
      "Downloading scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "Successfully installed scipy-1.16.3\n",
      "Click probability in control group:  0.1992\n",
      "Click probability in Experimental group:  0.4915\n",
      "p_pooled hat is 0.34535\n",
      "p pooled variance is: 4.52166755e-05\n",
      "Standard Error is:  0.006724334576744378\n",
      "Test Statistics for 2-sample Z-test is: -43.46898517080014\n",
      "Alpha: significance level is:  0.05\n",
      "Z-critical value from the standard normal distribution:  1.959963984540054\n",
      "P-value of the 2-sample Z-test:  0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "N_exp = 10000\n",
    "N_con = 10000\n",
    "\n",
    "click_exp = pd.Series(np.random.binomial(1,0.5, size = N_exp))\n",
    "click_con = pd.Series(np.random.binomial(1,0.2, size = N_con))\n",
    "\n",
    "exp_id = pd.Series(np.repeat(\"exp\", N_exp))\n",
    "con_id = pd.Series(np.repeat(\"con\", N_con))\n",
    "\n",
    "df_exp = pd.concat(([click_exp, exp_id]), axis = 1)\n",
    "df_con = pd.concat(([click_con, con_id]), axis = 1)\n",
    "\n",
    "df_exp.columns = [\"click\", \"group\"]\n",
    "df_con.columns = [\"click\", \"group\"]\n",
    "\n",
    "df_ab_test = pd.concat([df_exp, df_con], axis = 0).reset_index(drop=True)\n",
    "\n",
    "X_con = df_ab_test.groupby(\"group\")[\"click\"].sum().loc[\"con\"]\n",
    "X_exp = df_ab_test.groupby(\"group\")[\"click\"].sum().loc[\"exp\"]\n",
    "\n",
    "p_con_hat = X_con / N_con\n",
    "p_exp_hat = X_exp / N_exp\n",
    "print(\"Click probability in control group: \", p_con_hat)\n",
    "print(\"Click probability in Experimental group: \", p_exp_hat)\n",
    "\n",
    "p_pooled_hat = (X_con+X_exp)/(N_con+N_exp)\n",
    "print(\"p_pooled hat is\", p_pooled_hat)\n",
    "\n",
    "pooled_variance = p_pooled_hat * (1-p_pooled_hat) * (1/N_con + 1/N_exp)\n",
    "print(\"p pooled variance is:\", pooled_variance)\n",
    "\n",
    "SE = np.sqrt(pooled_variance)\n",
    "print(\"Standard Error is: \", SE)\n",
    "\n",
    "Test_stat = (p_con_hat - p_exp_hat)/SE\n",
    "print(\"Test Statistics for 2-sample Z-test is:\", Test_stat)\n",
    "\n",
    "alpha = 0.05\n",
    "print(\"Alpha: significance level is: \", alpha)\n",
    "\n",
    "# Calculating the rejection region\n",
    "Z_crit = norm.ppf(1-alpha/2)\n",
    "print(\"Z-critical value from the standard normal distribution: \", Z_crit)\n",
    "\n",
    "p_value = 2 * norm.sf(abs(Test_stat))\n",
    "print(\"P-value of the 2-sample Z-test: \", round(p_value, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab908bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline conversion rate: (the current rate at which users take the desired action)\n",
    "baseline_conversion_rate = 0.05\n",
    "\n",
    "variation_of_population = \n",
    "# Minimum desired effect (The smallest improvement that is worth your time and effort to detect, eg do we change from 5% to 5.01% - we probably dont care. From 5-6% thats a 20% relative increase) \n",
    "# Here we want to use the absolute increase instead\n",
    "delta = 0.1\n",
    "\n",
    "# Statistical significance (alpha) -> confidence level, the % that we will reject the null when there is inface no change\n",
    "alpha = 0.05\n",
    "\n",
    "# Statstical power (1 - beta) -> probability that if there is a real difference our test will actually find it\n",
    "beta = 0.8\n",
    "\n",
    "\n",
    "# Calculating the rejection region\n",
    "Z_crit_sig = norm.ppf(1-alpha/2)\n",
    "Z_crit_power = norm.ppf(1-beta/2)\n",
    "sample_size_per_variation = ((variation_of_population ** 2) * (Z_crit_sig + Z_crit_power)**2) / delta**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37a7e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in ./.venv/lib/python3.12/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in ./.venv/lib/python3.12/site-packages (from statsmodels) (2.3.4)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in ./.venv/lib/python3.12/site-packages (from statsmodels) (1.16.3)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in ./.venv/lib/python3.12/site-packages (from statsmodels) (2.3.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in ./.venv/lib/python3.12/site-packages (from statsmodels) (1.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in ./.venv/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "You need 1565 users per group\n",
      "total traffic needed: 3130\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels\n",
    "import statsmodels.stats.api as sms\n",
    "from math import ceil\n",
    "\n",
    "baseline_rate = 0.45\n",
    "mde = 0.05\n",
    "alpha = 0.05\n",
    "power = 0.80\n",
    "\n",
    "effect_size = sms.proportion_effectsize(baseline_rate, baseline_rate+mde)\n",
    "\n",
    "required_n = sms.NormalIndPower().solve_power(\n",
    "    effect_size = effect_size,\n",
    "    power=power,\n",
    "    alpha=alpha,\n",
    "    ratio=1\n",
    ")\n",
    "\n",
    "required_n = ceil(required_n)\n",
    "print(f\"You need {required_n} users per group\")\n",
    "print(f\"total traffic needed: {required_n *2 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef385df",
   "metadata": {},
   "source": [
    "Known sigma (standard diviation of the population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5693fe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size = 63 for both groups\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats as st\n",
    "# Mean of group 1, μ_1\n",
    "mu_1 = 5\n",
    "# Mean of group 2, μ_2\n",
    "mu_2 = 10\n",
    "# Sampling ratio, κ = n_1 / n_2\n",
    "kappa = 1\n",
    "# Population standard deviation, σ\n",
    "sigma = 10\n",
    "# Type I error rate, α\n",
    "alpha = 0.05\n",
    "# Type II error rate, β\n",
    "beta = 0.20\n",
    "\n",
    "# Sample size estimation\n",
    "n_2 = (1 + 1 / kappa) * (\n",
    "    sigma * (\n",
    "        st.norm.ppf(1 - alpha / 2) + st.norm.ppf(1 - beta)\n",
    "    ) / (mu_1 - mu_2)\n",
    ")**2\n",
    "n_2 = np.ceil(n_2)\n",
    "\n",
    "print(f'Sample size = {n_2:n} for both groups')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11baaa20",
   "metadata": {},
   "source": [
    "Unknown sigma (standard deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e74c5480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group (means): 23\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats as st\n",
    "import math\n",
    "\n",
    "def calculate_sample_size_means(mu_1, mu_2, sigma, alpha=0.05, beta=0.20, kappa=1):\n",
    "    '''\n",
    "    Calculate sample size for comparing two means (continous data)\n",
    "    '''\n",
    "    # 1. Get the Critical Z-Vaues\n",
    "    # For 95% confidence (alpha=0.05), we need the 97.5th percentile (two tailed)\n",
    "    z_alpha = st.norm.ppf(1-beta)\n",
    "\n",
    "    # For 80% power (beta=0.20), we need the 80th percentile (one tailed)\n",
    "    z_beta = st.norm.ppf(1-beta)\n",
    "\n",
    "    # 2. The numerator: (Sigma * (Z_alpha + Z_beta))\n",
    "    # This represents the total spread we need to cover to distinguish the signal\n",
    "    numerator = sigma * (z_alpha + z_beta)\n",
    "\n",
    "    # 3. The Denominator: The minimal detectable difference\n",
    "    denominator = mu_1 - mu_2\n",
    "\n",
    "    # 4. The formula:\n",
    "    n_2 = (1+ 1/kappa)*(numerator / denominator)**2\n",
    "\n",
    "    # 5. Round up to the ceiling because you cant have half a user\n",
    "    return math.ceil(n_2)\n",
    "\n",
    "mu_1 = int(input(\"What is the mean of group 1: \"))\n",
    "mu_2 = int(input(\"What is the mean of group 2: \"))\n",
    "sigma = int(input(\"What is the standard deviation: \"))\n",
    "\n",
    "required_n = calculate_sample_size_means(mu_1=mu_1, mu_2=mu_2, sigma=sigma)\n",
    "\n",
    "print(f\"Required sample size per group (means): {required_n}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66279d9c",
   "metadata": {},
   "source": [
    "# Proportions / Conversion rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88dfa7fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The required sample size per group (proportions): 321\n"
     ]
    }
   ],
   "source": [
    "def calculate_sample_size_proportions(baseline_rate, mde, alpha=0.05, beta=0.20, kappa=1):\n",
    "    '''\n",
    "    Calculate the sample size for comparing two proportions (conversion rates)\n",
    "    Calculated from scratch using the pooled variance method'''\n",
    "\n",
    "    # 1. Define p1 and p2\n",
    "    p1 = baseline_rate\n",
    "    p2 = baseline_rate + mde\n",
    "\n",
    "    # 2. Pooled Probability (average of the two rates)\n",
    "    p_avg = (p1 + p2) / 2\n",
    "\n",
    "    # 3. Get critical z-values\n",
    "    z_alpha = st.norm.ppf(1 - alpha / 2)\n",
    "    z_beta = st.norm.ppf(1 - beta)\n",
    "\n",
    "    # 4. Calculate the Variance Components\n",
    "    # Standard error depends on the variance of both groups\n",
    "    # Variance = p * (1-p)\n",
    "    # We sum the variances (multipled by 2 for equal groups approximation)\n",
    "    # A common standard approximation is 2 * p_avg * (1 - p_avg)\n",
    "    pooled_variance = 2 * p_avg * (1 - p_avg)\n",
    "\n",
    "    # 5. The formula:\n",
    "    numerator = pooled_variance * (z_alpha + z_beta)**2\n",
    "    denominator = (p2 - p1) ** 2\n",
    "\n",
    "    n = numerator / denominator\n",
    "\n",
    "    return math.ceil(n)\n",
    "\n",
    "baseline_rate = float(input(\"please enter the baseline rate of conversion: \"))\n",
    "mde = float(input(\"please enter the minimal desired absolute change / effect: \"))\n",
    "n_proportions = calculate_sample_size_proportions(baseline_rate=baseline_rate, mde=mde)\n",
    "print(f\"The required sample size per group (proportions): {n_proportions}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
